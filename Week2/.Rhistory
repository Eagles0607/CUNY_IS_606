labs(title <- "Transaction Values per Transaction Date")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
sum(!complete.cases(canada_transactions))
sum(complete.cases(canada_transactions))
training.data.set <- read.csv("users/bcarancibia/CUNY_IS_621/Week2/data/coin-training-data.csv",header=T,stringsAsFactors=F)
training.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/coin-training-data.csv",header=T,stringsAsFactors=F)
w <- c(0,0,0,0)
training.data.set$id <- rep(1,length(training.data.set$id))
colnames(training.data.set)[1] <- "intercept"
training.data.set$class <- ifelse(training.data.set$answer=="cent",1,-1)
inputs <- as.matrix(training.data.set[,1:4])
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(1, 0, 1, 0)
a <- 0.5
perceptron(inputs, y, w, a, verbose=T)
perceptron <- function(x, y, w, a, verbose=F) {
hyp <- function(w, x) {
p <- w %*% x
sign(p - abs(p)) + 1
}
iter <- function(x, y, w, a) {
w + a * (y - hyp(w, x)) * x
}
wold <- matrix(w, nrow=1)
nrows <- dim(x)[1]
done <- FALSE
iteration <- 0
while (! done) {
if (verbose) {
print(paste("Iteration", iteration, sep=" "), quote=F)
print(wold)
}
iteration <- iteration + 1
count <- 0
for (i in 1:nrows) {
wnew = iter(x[i,], y[i], wold, a)
if (all(wnew == wold)) {
count <- count + 1
}
wold <- wnew
}
if (count == nrows) {
done <- TRUE
}
}
wnew
}
perceptron(inputs, y, w, a, verbose=T)
perceptron_scratch <- function(w, X, y, eta, max.epoch){
N <- nrow(X)/2;
epoch <- 0;
repeat {w.old <- w;
for (i in 1:(2*N)) {
if ( t[i]*y(X[i,],w) <= 0 )
w <- w + eta * t[i] * X[i,];
}
epoch <- epoch + 1;
if ( identical(w.old,w) || epoch = max.epoch ) {
break;
}
}
return (w);
}
perceptron_scratch <- function(w, X, y, eta, max.epoch){
N <- nrow(X)/2;
epoch <- 0;
repeat {
w.old <- w;
for (i in 1:(2*N)) {
if ( t[i]*y(X[i,],w) <= 0 )
w <- w + eta * t[i] * X[i,];
}
epoch <- epoch + 1;
if ( identical(w.old,w) || epoch = max.epoch ) {
break;
}
}
return (w);
}
perceptron_scratch <- function(w, X, y, eta, max.epoch){
N <- nrow(X)/2;
epoch <- 0;
repeat {
w.old <- w;
for (i in 1:(2*N)) {
if ( t[i]*y(X[i,],w) <= 0 )
w <- w + eta * t[i] * X[i,];
}
epoch <- epoch + 1;
if ( identical(w.old,w) || epoch = max.epoch ) {
break;
}
}
return (w);
}
perceptron_scratch <- function(w, X, y, eta, max.epoch){
N <- nrow(X)/2;
epoch <- 0;
repeat {
w.old <- w;
for (i in 1:(2*N)) {
if ( t[i]*y(X[i,],w) <= 0 )
w <- w + eta * t[i] * X[i,];
}
epoch <- epoch + 1;
if ( identical(w.old,w) || epoch = max.epoch ) {
break;
}
}
return (w);
}
perceptron(x, y, w, a, verbose=T)
#training.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/test-public.csv", header = TRUE)
training.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/coin-training-data.csv",header=T,stringsAsFactors=F)
#test.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/test-private.csv", header = TRUE)
perceptron(inputs, y, w, a, verbose=T)
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(1, 0, 1, 0)
a <- 0.5
perceptron(x, y, w, a, verbose=T)
w <- c(0,0,0)
w <- c(0,0,0)
perceptron(x, y, w, a, verbose=T)
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(-1, 1, -1,1)
a <- 0.5
perceptron(x, y, w, a, verbose=T)
perceptron(inputs, y, w, a, verbose=T)
w <- c(0,0,0,0)
perceptron(inputs, y, w, a, verbose=T)
bivariate <- function(x,y){
term1 <- 1 / (2 * pi * sig1 * sig2 * sig3 * sqrt(1 - rho^2))
term2 <- (x - mu1)^2 / sig1^2
term3 <- -(2 * rho * (x - mu1)*(y - mu2))/(sig1 * sig2)
term4 <- (y - mu2)^2 / sig2^2
z <- term2 + term3 + term4
term5 <- term1 * exp((-z / (2 *(1 - rho^2))))
return (term5)
}
x <- seq(xm, xp, length= as.integer((xp + abs(xm)) * 10))  # vector series x
y <- seq(ym, yp, length= as.integer((yp + abs(ym)) * 10))  # vector series y
mu1 <- 2.5 # expected value of mass
mu2 <- 19.05	# expected value of diameter
mu3 <- 1.52 #thickness
sig1 <- 0.0025	# variance of mass
sig2 <- 0.1452 # variance of diameter
sig3 <- 0.0009
rho <- 0.5	# corr(x, y)
# Some additional variables for x-axis and y-axis
xm <- -3
xp <- 3
ym <- -3
yp <- 3
x <- seq(xm, xp, length= as.integer((xp + abs(xm)) * 10))  # vector series x
y <- seq(ym, yp, length= as.integer((yp + abs(ym)) * 10))  # vector series y
# Core function
bivariate <- function(x,y){
term1 <- 1 / (2 * pi * sig1 * sig2 * sig3 * sqrt(1 - rho^2))
term2 <- (x - mu1)^2 / sig1^2
term3 <- -(2 * rho * (x - mu1)*(y - mu2))/(sig1 * sig2)
term4 <- (y - mu2)^2 / sig2^2
z <- term2 + term3 + term4
term5 <- term1 * exp((-z / (2 *(1 - rho^2))))
return (term5)
}
# Computes the density values
z <- outer(x,y,bivariate)
bivariate <- function(x,y){
term1 <- 1 / (2 * pi * sig1 * sig2 * sig3 * sqrt(1 - rho^2))
term2 <- (x - mu1)^2 / sig1^2
term3 <- -(2 * rho * (x - mu1)*(y - mu2))/(sig1 * sig2)
term4 <- (y - mu2)^2 / sig2^2
z <- term2 + term3 + term4
term5 <- term1 * exp((-z / (2 *(1 - rho^2))))
return (term5)
}
z <- outer(x,y,bivariate)
# Plot
persp(x, y, z, main = "Bivariate Normal Distribution",
sub = bquote(bold(mu[1])==.(mu1)~", "~sigma[1]==.(sig1)~", "~mu[2]==.(mu2)~
", "~sigma[2]==.(sig2)~", "~rho==.(rho)),
col="orchid2", theta = 55, phi = 30, r = 40, d = 0.1, expand = 0.5,
ltheta = 90, lphi = 180, shade = 0.4, ticktype = "detailed", nticks=5)
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
methods(mvnorm)
library(MASS)
library(mass)
install.packages("mvtnorm")
methods(mvtnorm)
library(mvtnorm)
methods(mvtnorm)
dvnorm
dmvnorm
setwd("/users/bcarancibia/CUNY_IS_606/Week2")
require(MASS)
library(mvtnorm)
Sigma <- matrix(c(0.0025, 0, 0, 0, 0.1452, 0, 0, 0, 0.0009), ncol=3) # covariance matrix of X
sigma
Sigma
mu <- c(2.5, 19.05, 1.52)
Sigma <- matrix(c(0.0025, 0, 0, 0, 0.1452, 0, 0, 0, 0.0009), ncol=3) # covariance matrix of X
plot1 <- rmvnorm(n=500, mean=c(2.5, 19.05, 1.52), Sigma=sigma)
colMeans(plot1)
var(plot1)
plot(plot1)
plot1 <- rmvnorm(n=500, mean=c(2.5, 19.05, 1.52), sigma=Sigma)
colMeans(plot1)
var(plot1)
plot(plot1)
mu2 <- c(2.268, 17.91, 1.35)
Sigma2 <- matrix(c(0.0021, 0, 0, 0, 0.1283 , 0, 0, 0, 0.0007), ncol=3) # covariance matrix of X
plot2 <- rmvnorm(n=500, mean=c(2.268, 17.91, 1.35), sigma=Sigma2)
colMeans(plot2)
var(plot2)
plot(plot2)
public.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/test-public.csv", header = TRUE)
training.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/coin-training-data.csv",header=T,stringsAsFactors=F)
private.data.set <- read.csv("/users/bcarancibia/CUNY_IS_621/Week2/data/test-private.csv", header = TRUE)
multi_norm <- function(data,sample_num,R) {
# initial covariance matrix
V <- matrix(data=NA, nrow=data, ncol=data)
# mean for each gene
meansmodule <- runif(data, min=0, max=4)
# variance for each gene
varsmodule <- runif(data, min=0, max=4)
for (i in 1:data) {
# a two-level nested loop to generate covariance matrix
for (j in 1:data) {
if (i == j) {
# covariances on the diagonal
V[i,j] <- varsmodule[i]
} else {
# covariances
V[i,j] <- R * sqrt(varsmodule[i]) * sqrt(varsmodule[j])
}
}
}
# simulate multivariate normal distribution
# given means and covariance matrix
X <- t(mvrnorm(n = sample_num, meansmodule, V))
return(X)
}
multi_norm(public.data.set, 500, R)
View(public.data.set)
View(training.data.set)
inputs <- as.matrix(training.data.set[,1:4])
inputs2 <- as.matrix(public.data.set[,1:4])
inputs3 <- as.matrix(private.data.set[,1:4])
multi_norm(inputs2, 500, R)
require(MASS)
multi_norm(inputs2, 200, R)
multi_norm <- function(data,sample_num,R) {
# initial covariance matrix
V <- matrix(data=NA, nrow=data, ncol=data)
# mean for each gene
meansmodule <- runif(data, min=0, max=4)
# variance for each gene
varsmodule <- runif(data, min=0, max=4)
for (i in 1:data) {
# a two-level nested loop to generate covariance matrix
for (j in 1:data) {
if (i == j) {
# covariances on the diagonal
V[i,j] <- varsmodule[i]
} else {
# covariances
V[i,j] <- R * sqrt(varsmodule[i]) * sqrt(varsmodule[j])
}
}
}
# simulate multivariate normal distribution
# given means and covariance matrix
X <- t(mvtnorm(n = sample_num, meansmodule, V))
return(X)
}
multi_norm(inputs2, 200, R)
require(mvtnorm)
multi_norm(inputs2, 200, R)
#c)
multi_norm <- function(data,sample_num,R) {
# initial covariance matrix
V <- matrix(data=NA, nrow=data, ncol=data)
# mean for each gene
meansmodule <- runif(data, min=0, max=4)
# variance for each gene
varsmodule <- runif(data, min=0, max=4)
for (i in 1:data) {
# a two-level nested loop to generate covariance matrix
for (j in 1:data) {
if (i == j) {
# covariances on the diagonal
V[i,j] <- varsmodule[i]
} else {
# covariances
V[i,j] <- R * sqrt(varsmodule[i]) * sqrt(varsmodule[j])
}
}
}
# simulate multivariate normal distribution
# given means and covariance matrix
X <- t(mvtnorm(n = sample_num, meansmodule, V))
return(V)
}
multinorm(inputs2, 200, R)
multi_norm(inputs2, 200, R)
multi_norm <- function(data,sample_num,R) {
# initial covariance matrix
V <- matrix(data=NA, nrow=data, ncol=data)
# mean for each gene
meansmodule <- runif(data, min=0, max=4)
# variance for each gene
varsmodule <- runif(data, min=0, max=4)
for (i in 1:data) {
# a two-level nested loop to generate covariance matrix
for (j in 1:data) {
if (i == j) {
# covariances on the diagonal
V[i,j] <- varsmodule[i]
} else {
# covariances
V[i,j] <- R * sqrt(varsmodule[i]) * sqrt(varsmodule[j])
}
}
}
# simulate multivariate normal distribution
# given means and covariance matrix
#X <- t(mvtnorm(n = sample_num, meansmodule, V))
return(V)
}
multi_norm(inputs2, 200, R)
cov(public.data.set, y = NULL, use = "everything",
method = c("pearson", "kendall", "spearman"))
cov(public.data.set, y = NULL, use = [,1:4],
method = c("pearson", "kendall", "spearman"))
cov(public.data.set, y = NULL, use = [,1:4],
method = c("pearson", "kendall", "spearman"))
cov(public.data.set, y = NULL, use = [,1:4],
method = c("pearson", "kendall", "spearman")
inputs2
inputs4 <- as.matrix(public.data.set[,2:4])
inputs4
cov(inputs4, y = NULL, use = "everything",
method = c("pearson", "kendall", "spearman"))
#c)
mu3 <- colMeans(public.data.set)
Sigma3 <- cov(inputs4, y = NULL, use = "everything", method = c("pearson", "kendall", "spearman"))
mu3 <- colMeans(inputs4)
Sigma3 <- cov(inputs4, y = NULL, use = "everything", method = c("pearson", "kendall", "spearman"))
Sigma3 <- cov(inputs4, y = NULL, use = "everything", method = c("pearson", "kendall", "spearman"))
plot3 <- rmvnorm(n=500, mean=mu3, sigma=Sigma3)
colMeans(plot3)
var(plot3)
plot(plot3)
mu3 <- colMeans(inputs4)
Sigma3 <- cov(inputs4, y = NULL, use = "everything", method = c("pearson", "kendall", "spearman"))
plot3 <- rmvnorm(n=500, mean=mu3, sigma=Sigma3)
colMeans(plot3)
var(plot3)
plot(plot3)
View(public.data.set)
df <- public.data.set[-1]
df$answer <- factor(df$answer, levels=c(0,1), labels=c("cent", "dime"))
train <- sample(nrow(df, 0.7*nrow(df)))
df.train <- df[train,]
df.validate <- df[-train,]
table(df.train$answer)
table(df.validate$answer)
train <- sample(nrow(df), 0.7*nrow(df)))
df.train <- df[train,]
df.validate <- df[-train,]
table(df.train$answer)
table(df.validate$answer)
train <- sample(nrow(df), 0.7*nrow(df))
df.train <- df[train,]
df.validate <- df[-train,]
table(df.train$answer)
table(df.validate$answer)
fit.logit <- glm(answer~. , data=df.train)
fit.logit <- glm(answer~. , data=df.train, family=binomial())
summary(fit.logit)
prob <- predict(fit.logit, df.validate, type="response")
fit.logit <- glm(answer~. , data=df.train, family=binomial())
summary(fit.logit)
prob <- predict(fit.logit, df.validate, type="response")
logit.pred <- factor(prob > .5, levels=c(FALSE, TRUE), labels=c("cent", "dime"))
logit.perf <- table(df.validate$answer, logit.pred,dnn=c("Actual", "Predicted"))
logit.perf
View(private.data.set)
df2 <- private.data.set[-1]
df2$answer <- factor(df2$answer, levels=c(0,1), labels=c("cent", "dime"))
train2 <- sample(nrow(df2), 0.7*nrow(df2))
df2.train <- df[train2,]
df2.validate <- df[-train2,]
table(df2.train$answer)
table(df2.validate$answer)
fit.logit2 <- glm(answer~. , data=df2.train, family=binomial())
summary(fit.logit2)
prob2 <- predict(fit.logit2, df2.validate, type="response")
logit.pred2 <- factor(prob > .5, levels=c(FALSE, TRUE), labels=c("cent", "dime"))
logit.perf2 <- table(df2.validate$answer, logit.pred,dnn=c("Actual", "Predicted"))
logit.perf2
perceptron <- function(x, y, w, a, verbose=F) {
hyp <- function(w, x) {
p <- w %*% x
sign(p - abs(p)) + 1
}
iter <- function(x, y, w, a) {
w + a * (y - hyp(w, x)) * x
}
wold <- matrix(w, nrow=1)
nrows <- dim(x)[1]
done <- FALSE
iteration <- 0
while (! done) {
if (verbose) {
print(paste("Iteration", iteration, sep=" "), quote=F)
print(wold)
}
iteration <- iteration + 1
count <- 0
for (i in 1:nrows) {
wnew = iter(x[i,], y[i], wold, a)
if (all(wnew == wold)) {
count <- count + 1
}
wold <- wnew
}
if (count == nrows) {
done <- TRUE
}
}
wnew
}
# Example
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(-1, 1, -1,1)
w <- c(0,0,0,0)
a <- 0.5
perceptron(inputs, y, w, a, verbose=T)
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(-1, -1, 1,1)
w <- c(0,0,0,0)
a <- 0.5
perceptron(inputs, y, w, a, verbose=T)
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(-1, -1, 1,1)
w <- c(0,0,0,0)
a <- 1
perceptron(inputs, y, w, a, verbose=T)
rosen <- function(x, y, beta0=1, tol=1e-8, minepochs=2, maxepochs=100, verbose=FALSE)
{
N <- nrow(x)
p <- ncol(x)
if (length(y) != N) {
stop("The number of rows in x is not the length of y")
}
if (length(beta0==1))
beta0=rep(beta0,p)
if (length(beta0) != p) {
stop("The dimension of beta0 is not the number of columns in x")
}
# make it stochastic!
o <- sample(N)
x <- x[o,]
y <- y[o]
itlim <- maxepochs * N
itmin <- minepochs * N
eps <- .Machine$double.xmax
beta <- beta0
i <- 0;
while ((eps > tol && i < itlim) || (i < itmin)) {
beta0 <- beta
ind <-  (i %% N) + 1
# check if this point is misclassified
d <- y[ind] * crossprod(beta, x[ind,])
if (d < 0) {
# it is, update beta
beta <- beta0 + y[ind] * x[ind,]
}
# update epsilon
eps <- sqrt(sum((beta0-beta)^2))/sqrt(sum(beta0^2))
i <- i+1
if (verbose)
cat(sprintf("It: %d ind:%d d:%.4f eps: %.4f\n", i, ind, d, eps))
}
if (i==itlim)
warning("Failed to converge")
return(beta)
}
rosen(inputs,y)
rosen(inputs)
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(1,1,1,1)
w <- c(0,0,0,0)
a <- 1
perceptron(inputs, y, w, a, verbose=T)
x <- matrix(c(1, 2, 1, 2, 2, 1, 4, 1, 1, 5, 1, 1), nrow=4, byrow=T)
y <- c(-1,1,1,1)
w <- c(0,0,0,0)
a <- 1
perceptron(inputs, y, w, a, verbose=T)
install.packages("Shiny")
install.packages("shiny")
library(shiny)
runExample("01_hello")
runExample("02_text")
runExample("03_reactivity")
shiny::runApp('~/s_homework/shinyapp')
