---
title: "IS 606: Statistics and Probability for Data Analytics Final Exam"
author: "Ben Arancibia"
date: "May 22, 2015"
output: html_document
---

---
title: 'IS 606: Statistics and Probability for Data Analytics Final Exam'
author: "Ben Arancibia"
date: "May 22, 2015"
output: pdf_document
---

#Binomial or Hypergeometric

1) Binomial distribution to calculate the probability that there will be three success out of ten trials, probability of success is 0.30

```{r}
dbinom(3, 10, 0.3) #dbinom(k, n, p)
```

2) Expected value and the standard deviation for binomial experiment with ten trials and P = 0.30

Expected value = mean

n*p = expected value

```{r}
10*.30
```

Standard deviation is sqrt of variance.

```{r}
sqrt(10*.3*(1-.3)) 
```

3) Hypergeometric distribution to calculate the probability that there will be three success out of ten trials, n = 20 with eight possible successes. 

```{r}
dhyper(3, 8, 12, 10) #dhyper(x, success, n-sucess, trials) (x,m,n,k)
```

4) Expected value and the standard deviation for a hypergeometric experiment with ten trials, n=20 with eight possible successes

Expected value = n*(k/ N)

```{r}
10*(8/20)
```

Standard deviation

```{r}
sqrt(10*(8/20)*(1-(8/20))*((20-10)/(20-1)))
```

5) hypergeometric distribution to calculate the probability that there will be three success out of ten trials, N=60 with 24 possible successes.

```{r}
dhyper(3, 24, 36, 10)
```

6) Expected value and the Standard deviation for a hypergeometric experiment with ten trials, N=60 with 24 possible successes

Expected value = n*(k/ N)
```{r}
10*(24/36)
```

Standard deviation

```{r}
sqrt(10*(24/60)*(1-(24/60))*((60-20)/(60-1)))
```

7) What population size does the binomial start to make a strong approximation for the hypergeometric distribution?

The Hypergeometric(n, D, M) can be approximated by Binomial(n, D/M). The approximate works well when n < 0.1 M

As long as n is less 0.1 of success trials

#Conditional Probability and the Naive Bayes Classification Method

1) The key assumption that makes Naïve Bayes feasible is that the predictor variables are independent of one another. Use the definitions of independence and conditional probability to show that, under the assumption that each of the predictor variables X1,X2,X3 is independent of the others given the class yi of the observation.

P(X1 = x1 & X2 = x2 & X3 = x3 | Y = yi) = P(X1 =x1 |Y=yi) * P(X2=x2|Y=yi) * P(X3=x3|Y=yi)

Assume j != i you can use the chain rule so that x1 * x2 * x3 * y = ((x1 * y) (x2 * y) (x3 * y))


2) 

P(H│E)=(P(H)P(E|H))/(P(H)P(E│H)+P(not H)P(E|not H))

derive a formula for P(Y=yi | X1=x1  & X2=x2  & X3=x3 ). Assume that there are only two classifications Y=y1 and Y=y2 and that their prior probabilities are known.

P(x|y1 * y2) = (P(y1|x) * (P(y2|x) * P(x) ) / P(x)

3) 

P(x|y1) = (P(y1|x) * (P(y2|x) *  (P(y3|x) * P(x) ) / P(x)

4) 

It is unnecessary to calculate the entire denominator when applying this classification technique because in practice the denominator does not depend on the changing variables and the denominator is effectively constant

#Bayesian Techniques

1) Use binomial distribution

```{r}
dbinom(4, 5, 0.5) #dbinom(k, n, p)
```

The probability that it is a fair coin is 15% and the probability that the coin is not fair is 85%.

2)
```{r}
dbinom(7,8,0)
dbinom(7,8,.25)
dbinom(7,8,0.5)
dbinom(7,8,.75)
dbinom(7,8,1)
```

![Table 2](/users/bcarancibia/CUNY_IS_606/Final/question2.png)

3) What the results mean for p=0 and p=1 in the table above means that those two parameters are not possible based on the results. The result makes sense given the evidence observed. These probabilities will never change again based on the new evidence because the likelihood is 0 based on the calculated results.

4) Make a new table, using your posterior probabilities from above as your new priors, and update your probabilities using this new evidence.

```{r}
dbinom(5,6,.25)
dbinom(5,6,.50)
dbinom(5,6,.75)
```

![Table 2](/users/bcarancibia/CUNY_IS_606/Final/question3.png)


5)

```{r}
dbinom(12,14,0)
dbinom(12,14,.25)
dbinom(12,14,.50)
dbinom(12,14,.75)
dbinom(12,14,1)
```

![Table 5](/users/bcarancibia/CUNY_IS_606/Final/question5.png)


The priors are different than the previous part, but the posteriors are the same. What this means is that the someone can do this calculation in an aggregate instead of running experiments. This makes sense because of how the Naive Bayes Classifer has both independence and conditional probability as part of the formula.  
